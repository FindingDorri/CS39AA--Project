{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning a Pre-trained Model","metadata":{}},{"cell_type":"markdown","source":"Since our dataset is large(>10K), we'll free layers close to input and train the later layers.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification,  TrainingArguments, Trainer\nfrom datasets import Dataset, Image, Value, ClassLabel, Features\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\nbase_dir = \"../input/dogsvscatsredux\"\ntrain_dir = os.path.join(base_dir, \"train/train\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:13.950584Z","iopub.execute_input":"2022-12-04T07:21:13.950950Z","iopub.status.idle":"2022-12-04T07:21:20.050486Z","shell.execute_reply.started":"2022-12-04T07:21:13.950918Z","shell.execute_reply":"2022-12-04T07:21:20.049288Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pd.set_option('max_colwidth', 60)\ntrain_filenames = os.listdir(base_dir+ '/train/train')\ntrain_data = pd.DataFrame(columns=['image', 'labels'])\ntrain_data['image'] = train_filenames \ntrain_data['labels'] = train_data['image'].str[0:3].map({\"cat\":0,\"dog\":1})\ntrain_data['image'] = (train_dir +\"/\" +train_data['image']).astype(str)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.056066Z","iopub.execute_input":"2022-12-04T07:21:20.056806Z","iopub.status.idle":"2022-12-04T07:21:20.117530Z","shell.execute_reply.started":"2022-12-04T07:21:20.056759Z","shell.execute_reply":"2022-12-04T07:21:20.116615Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                image  labels\n0  ../input/dogsvscatsredux/train/train/cat.12461.jpg       0\n1   ../input/dogsvscatsredux/train/train/dog.3443.jpg       1\n2   ../input/dogsvscatsredux/train/train/dog.7971.jpg       1\n3  ../input/dogsvscatsredux/train/train/dog.10728.jpg       1\n4   ../input/dogsvscatsredux/train/train/dog.1942.jpg       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/dogsvscatsredux/train/train/cat.12461.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/dogsvscatsredux/train/train/dog.3443.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/dogsvscatsredux/train/train/dog.7971.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/dogsvscatsredux/train/train/dog.10728.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/dogsvscatsredux/train/train/dog.1942.jpg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"features = Features({\"image\": Value(\"string\"), \"labels\": ClassLabel(num_classes=2, names=[\"cat\",\"dog\"])})\nds_raw = Dataset.from_pandas(train_data[['labels','image']],features=features).cast_column('image', Image())\nds_raw","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.121583Z","iopub.execute_input":"2022-12-04T07:21:20.122355Z","iopub.status.idle":"2022-12-04T07:21:20.148310Z","shell.execute_reply.started":"2022-12-04T07:21:20.122317Z","shell.execute_reply":"2022-12-04T07:21:20.147296Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'labels'],\n    num_rows: 25000\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor\n\nmodel_name_or_path = 'google/vit-base-patch16-224-in21k'\nfeature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.153690Z","iopub.execute_input":"2022-12-04T07:21:20.156374Z","iopub.status.idle":"2022-12-04T07:21:20.639586Z","shell.execute_reply.started":"2022-12-04T07:21:20.156314Z","shell.execute_reply":"2022-12-04T07:21:20.638515Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\nFeature extractor ViTFeatureExtractor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"size\": 224\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def transform(example_batch):\n    # Take a list of PIL images and turn them to pixel values\n    inputs = feature_extractor([x for x in example_batch[\"image\"]], return_tensors='pt')\n\n    # Don't forget to include the labels!\n    inputs[\"labels\"] = example_batch[\"labels\"]\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.643752Z","iopub.execute_input":"2022-12-04T07:21:20.644194Z","iopub.status.idle":"2022-12-04T07:21:20.655977Z","shell.execute_reply.started":"2022-12-04T07:21:20.644155Z","shell.execute_reply":"2022-12-04T07:21:20.654985Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"prepared_ds = ds_raw.with_transform(transform)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.660943Z","iopub.execute_input":"2022-12-04T07:21:20.663678Z","iopub.status.idle":"2022-12-04T07:21:20.671813Z","shell.execute_reply.started":"2022-12-04T07:21:20.663639Z","shell.execute_reply":"2022-12-04T07:21:20.670800Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['labels'] for x in batch])\n    }","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.676632Z","iopub.execute_input":"2022-12-04T07:21:20.679148Z","iopub.status.idle":"2022-12-04T07:21:20.686246Z","shell.execute_reply.started":"2022-12-04T07:21:20.679110Z","shell.execute_reply":"2022-12-04T07:21:20.685336Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric(\"accuracy\")\ndef compute_metrics(p):\n    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:20.690111Z","iopub.execute_input":"2022-12-04T07:21:20.692155Z","iopub.status.idle":"2022-12-04T07:21:21.353301Z","shell.execute_reply.started":"2022-12-04T07:21:20.692117Z","shell.execute_reply":"2022-12-04T07:21:21.352262Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We'll freeze the first two layers.","metadata":{}},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nlabels = ds_raw.features['labels'].names\n\nmodel = ViTForImageClassification.from_pretrained(\n    model_name_or_path,\n    num_labels=len(labels),\n    id2label={str(i): c for i, c in enumerate(labels)},\n    label2id={c: str(i) for i, c in enumerate(labels)}\n)\nfor name,param in model.named_parameters():\n    if name.startswith(\"vit.embeddings\"):\n        param.requires_grad = False\n    if any(x in name for x in ['.'+ str(x)+'.' for x in range (0,2)]): \n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:21.354681Z","iopub.execute_input":"2022-12-04T07:21:21.355371Z","iopub.status.idle":"2022-12-04T07:21:24.041083Z","shell.execute_reply.started":"2022-12-04T07:21:21.355329Z","shell.execute_reply":"2022-12-04T07:21:24.040279Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7bba26dd36a6ff9f6a9b19436dec361727bea03ec70fbfa82b70628109163eaa.92995a56e2eabab0c686015c4ad8275b4f9cbd858ed228f6a08936f2c31667e7\nModel config ViTConfig {\n  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"cat\",\n    \"1\": \"dog\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"cat\": \"0\",\n    \"dog\": \"1\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.20.1\"\n}\n\nloading weights file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/d01bfc4a52063e6f2cc1bc7063192e012043a7c6d8e75981bb6afbb9dc911001.e4710baf72bd00d091aab2ae692d487c057734cf044ba421696823447b95521e\nSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"for name,param in model.named_parameters():\n    print(name,param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:24.047092Z","iopub.execute_input":"2022-12-04T07:21:24.049550Z","iopub.status.idle":"2022-12-04T07:21:24.061871Z","shell.execute_reply.started":"2022-12-04T07:21:24.049509Z","shell.execute_reply":"2022-12-04T07:21:24.060779Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"vit.embeddings.cls_token False\nvit.embeddings.position_embeddings False\nvit.embeddings.patch_embeddings.projection.weight False\nvit.embeddings.patch_embeddings.projection.bias False\nvit.encoder.layer.0.attention.attention.query.weight False\nvit.encoder.layer.0.attention.attention.query.bias False\nvit.encoder.layer.0.attention.attention.key.weight False\nvit.encoder.layer.0.attention.attention.key.bias False\nvit.encoder.layer.0.attention.attention.value.weight False\nvit.encoder.layer.0.attention.attention.value.bias False\nvit.encoder.layer.0.attention.output.dense.weight False\nvit.encoder.layer.0.attention.output.dense.bias False\nvit.encoder.layer.0.intermediate.dense.weight False\nvit.encoder.layer.0.intermediate.dense.bias False\nvit.encoder.layer.0.output.dense.weight False\nvit.encoder.layer.0.output.dense.bias False\nvit.encoder.layer.0.layernorm_before.weight False\nvit.encoder.layer.0.layernorm_before.bias False\nvit.encoder.layer.0.layernorm_after.weight False\nvit.encoder.layer.0.layernorm_after.bias False\nvit.encoder.layer.1.attention.attention.query.weight False\nvit.encoder.layer.1.attention.attention.query.bias False\nvit.encoder.layer.1.attention.attention.key.weight False\nvit.encoder.layer.1.attention.attention.key.bias False\nvit.encoder.layer.1.attention.attention.value.weight False\nvit.encoder.layer.1.attention.attention.value.bias False\nvit.encoder.layer.1.attention.output.dense.weight False\nvit.encoder.layer.1.attention.output.dense.bias False\nvit.encoder.layer.1.intermediate.dense.weight False\nvit.encoder.layer.1.intermediate.dense.bias False\nvit.encoder.layer.1.output.dense.weight False\nvit.encoder.layer.1.output.dense.bias False\nvit.encoder.layer.1.layernorm_before.weight False\nvit.encoder.layer.1.layernorm_before.bias False\nvit.encoder.layer.1.layernorm_after.weight False\nvit.encoder.layer.1.layernorm_after.bias False\nvit.encoder.layer.2.attention.attention.query.weight True\nvit.encoder.layer.2.attention.attention.query.bias True\nvit.encoder.layer.2.attention.attention.key.weight True\nvit.encoder.layer.2.attention.attention.key.bias True\nvit.encoder.layer.2.attention.attention.value.weight True\nvit.encoder.layer.2.attention.attention.value.bias True\nvit.encoder.layer.2.attention.output.dense.weight True\nvit.encoder.layer.2.attention.output.dense.bias True\nvit.encoder.layer.2.intermediate.dense.weight True\nvit.encoder.layer.2.intermediate.dense.bias True\nvit.encoder.layer.2.output.dense.weight True\nvit.encoder.layer.2.output.dense.bias True\nvit.encoder.layer.2.layernorm_before.weight True\nvit.encoder.layer.2.layernorm_before.bias True\nvit.encoder.layer.2.layernorm_after.weight True\nvit.encoder.layer.2.layernorm_after.bias True\nvit.encoder.layer.3.attention.attention.query.weight True\nvit.encoder.layer.3.attention.attention.query.bias True\nvit.encoder.layer.3.attention.attention.key.weight True\nvit.encoder.layer.3.attention.attention.key.bias True\nvit.encoder.layer.3.attention.attention.value.weight True\nvit.encoder.layer.3.attention.attention.value.bias True\nvit.encoder.layer.3.attention.output.dense.weight True\nvit.encoder.layer.3.attention.output.dense.bias True\nvit.encoder.layer.3.intermediate.dense.weight True\nvit.encoder.layer.3.intermediate.dense.bias True\nvit.encoder.layer.3.output.dense.weight True\nvit.encoder.layer.3.output.dense.bias True\nvit.encoder.layer.3.layernorm_before.weight True\nvit.encoder.layer.3.layernorm_before.bias True\nvit.encoder.layer.3.layernorm_after.weight True\nvit.encoder.layer.3.layernorm_after.bias True\nvit.encoder.layer.4.attention.attention.query.weight True\nvit.encoder.layer.4.attention.attention.query.bias True\nvit.encoder.layer.4.attention.attention.key.weight True\nvit.encoder.layer.4.attention.attention.key.bias True\nvit.encoder.layer.4.attention.attention.value.weight True\nvit.encoder.layer.4.attention.attention.value.bias True\nvit.encoder.layer.4.attention.output.dense.weight True\nvit.encoder.layer.4.attention.output.dense.bias True\nvit.encoder.layer.4.intermediate.dense.weight True\nvit.encoder.layer.4.intermediate.dense.bias True\nvit.encoder.layer.4.output.dense.weight True\nvit.encoder.layer.4.output.dense.bias True\nvit.encoder.layer.4.layernorm_before.weight True\nvit.encoder.layer.4.layernorm_before.bias True\nvit.encoder.layer.4.layernorm_after.weight True\nvit.encoder.layer.4.layernorm_after.bias True\nvit.encoder.layer.5.attention.attention.query.weight True\nvit.encoder.layer.5.attention.attention.query.bias True\nvit.encoder.layer.5.attention.attention.key.weight True\nvit.encoder.layer.5.attention.attention.key.bias True\nvit.encoder.layer.5.attention.attention.value.weight True\nvit.encoder.layer.5.attention.attention.value.bias True\nvit.encoder.layer.5.attention.output.dense.weight True\nvit.encoder.layer.5.attention.output.dense.bias True\nvit.encoder.layer.5.intermediate.dense.weight True\nvit.encoder.layer.5.intermediate.dense.bias True\nvit.encoder.layer.5.output.dense.weight True\nvit.encoder.layer.5.output.dense.bias True\nvit.encoder.layer.5.layernorm_before.weight True\nvit.encoder.layer.5.layernorm_before.bias True\nvit.encoder.layer.5.layernorm_after.weight True\nvit.encoder.layer.5.layernorm_after.bias True\nvit.encoder.layer.6.attention.attention.query.weight True\nvit.encoder.layer.6.attention.attention.query.bias True\nvit.encoder.layer.6.attention.attention.key.weight True\nvit.encoder.layer.6.attention.attention.key.bias True\nvit.encoder.layer.6.attention.attention.value.weight True\nvit.encoder.layer.6.attention.attention.value.bias True\nvit.encoder.layer.6.attention.output.dense.weight True\nvit.encoder.layer.6.attention.output.dense.bias True\nvit.encoder.layer.6.intermediate.dense.weight True\nvit.encoder.layer.6.intermediate.dense.bias True\nvit.encoder.layer.6.output.dense.weight True\nvit.encoder.layer.6.output.dense.bias True\nvit.encoder.layer.6.layernorm_before.weight True\nvit.encoder.layer.6.layernorm_before.bias True\nvit.encoder.layer.6.layernorm_after.weight True\nvit.encoder.layer.6.layernorm_after.bias True\nvit.encoder.layer.7.attention.attention.query.weight True\nvit.encoder.layer.7.attention.attention.query.bias True\nvit.encoder.layer.7.attention.attention.key.weight True\nvit.encoder.layer.7.attention.attention.key.bias True\nvit.encoder.layer.7.attention.attention.value.weight True\nvit.encoder.layer.7.attention.attention.value.bias True\nvit.encoder.layer.7.attention.output.dense.weight True\nvit.encoder.layer.7.attention.output.dense.bias True\nvit.encoder.layer.7.intermediate.dense.weight True\nvit.encoder.layer.7.intermediate.dense.bias True\nvit.encoder.layer.7.output.dense.weight True\nvit.encoder.layer.7.output.dense.bias True\nvit.encoder.layer.7.layernorm_before.weight True\nvit.encoder.layer.7.layernorm_before.bias True\nvit.encoder.layer.7.layernorm_after.weight True\nvit.encoder.layer.7.layernorm_after.bias True\nvit.encoder.layer.8.attention.attention.query.weight True\nvit.encoder.layer.8.attention.attention.query.bias True\nvit.encoder.layer.8.attention.attention.key.weight True\nvit.encoder.layer.8.attention.attention.key.bias True\nvit.encoder.layer.8.attention.attention.value.weight True\nvit.encoder.layer.8.attention.attention.value.bias True\nvit.encoder.layer.8.attention.output.dense.weight True\nvit.encoder.layer.8.attention.output.dense.bias True\nvit.encoder.layer.8.intermediate.dense.weight True\nvit.encoder.layer.8.intermediate.dense.bias True\nvit.encoder.layer.8.output.dense.weight True\nvit.encoder.layer.8.output.dense.bias True\nvit.encoder.layer.8.layernorm_before.weight True\nvit.encoder.layer.8.layernorm_before.bias True\nvit.encoder.layer.8.layernorm_after.weight True\nvit.encoder.layer.8.layernorm_after.bias True\nvit.encoder.layer.9.attention.attention.query.weight True\nvit.encoder.layer.9.attention.attention.query.bias True\nvit.encoder.layer.9.attention.attention.key.weight True\nvit.encoder.layer.9.attention.attention.key.bias True\nvit.encoder.layer.9.attention.attention.value.weight True\nvit.encoder.layer.9.attention.attention.value.bias True\nvit.encoder.layer.9.attention.output.dense.weight True\nvit.encoder.layer.9.attention.output.dense.bias True\nvit.encoder.layer.9.intermediate.dense.weight True\nvit.encoder.layer.9.intermediate.dense.bias True\nvit.encoder.layer.9.output.dense.weight True\nvit.encoder.layer.9.output.dense.bias True\nvit.encoder.layer.9.layernorm_before.weight True\nvit.encoder.layer.9.layernorm_before.bias True\nvit.encoder.layer.9.layernorm_after.weight True\nvit.encoder.layer.9.layernorm_after.bias True\nvit.encoder.layer.10.attention.attention.query.weight True\nvit.encoder.layer.10.attention.attention.query.bias True\nvit.encoder.layer.10.attention.attention.key.weight True\nvit.encoder.layer.10.attention.attention.key.bias True\nvit.encoder.layer.10.attention.attention.value.weight True\nvit.encoder.layer.10.attention.attention.value.bias True\nvit.encoder.layer.10.attention.output.dense.weight True\nvit.encoder.layer.10.attention.output.dense.bias True\nvit.encoder.layer.10.intermediate.dense.weight True\nvit.encoder.layer.10.intermediate.dense.bias True\nvit.encoder.layer.10.output.dense.weight True\nvit.encoder.layer.10.output.dense.bias True\nvit.encoder.layer.10.layernorm_before.weight True\nvit.encoder.layer.10.layernorm_before.bias True\nvit.encoder.layer.10.layernorm_after.weight True\nvit.encoder.layer.10.layernorm_after.bias True\nvit.encoder.layer.11.attention.attention.query.weight True\nvit.encoder.layer.11.attention.attention.query.bias True\nvit.encoder.layer.11.attention.attention.key.weight True\nvit.encoder.layer.11.attention.attention.key.bias True\nvit.encoder.layer.11.attention.attention.value.weight True\nvit.encoder.layer.11.attention.attention.value.bias True\nvit.encoder.layer.11.attention.output.dense.weight True\nvit.encoder.layer.11.attention.output.dense.bias True\nvit.encoder.layer.11.intermediate.dense.weight True\nvit.encoder.layer.11.intermediate.dense.bias True\nvit.encoder.layer.11.output.dense.weight True\nvit.encoder.layer.11.output.dense.bias True\nvit.encoder.layer.11.layernorm_before.weight True\nvit.encoder.layer.11.layernorm_before.bias True\nvit.encoder.layer.11.layernorm_after.weight True\nvit.encoder.layer.11.layernorm_after.bias True\nvit.layernorm.weight True\nvit.layernorm.bias True\nclassifier.weight True\nclassifier.bias True\n","output_type":"stream"}]},{"cell_type":"code","source":"prepared_ds","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:24.063659Z","iopub.execute_input":"2022-12-04T07:21:24.064293Z","iopub.status.idle":"2022-12-04T07:21:24.070727Z","shell.execute_reply.started":"2022-12-04T07:21:24.064257Z","shell.execute_reply":"2022-12-04T07:21:24.069849Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'labels'],\n    num_rows: 25000\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_prop = 0.85\nds_train = prepared_ds.select(range(int(len(prepared_ds)*train_prop)))\nds_eval = prepared_ds.select(range(int(len(prepared_ds)*train_prop), len(prepared_ds)))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:24.073598Z","iopub.execute_input":"2022-12-04T07:21:24.074609Z","iopub.status.idle":"2022-12-04T07:21:24.095571Z","shell.execute_reply.started":"2022-12-04T07:21:24.074576Z","shell.execute_reply":"2022-12-04T07:21:24.094756Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:24.099205Z","iopub.execute_input":"2022-12-04T07:21:24.101215Z","iopub.status.idle":"2022-12-04T07:21:24.106697Z","shell.execute_reply.started":"2022-12-04T07:21:24.101181Z","shell.execute_reply":"2022-12-04T07:21:24.105919Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(num_train_epochs=1,\n                                  per_device_train_batch_size=10,\n                                  per_device_eval_batch_size=10,\n                                  evaluation_strategy=\"steps\",\n                                  eval_steps=300,\n                                  do_train=True,\n                                  report_to=None,\n                                  learning_rate= 5e-06, \n                                  output_dir=\"/kaggle/working\",\n                                  remove_unused_columns=False,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    train_dataset= ds_train,\n    eval_dataset= ds_eval,\n    tokenizer=feature_extractor,\n)\ntrain_results = trainer.train()\ntrainer.evaluate(ds_eval)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:21:24.110925Z","iopub.execute_input":"2022-12-04T07:21:24.113237Z","iopub.status.idle":"2022-12-04T07:37:34.732301Z","shell.execute_reply.started":"2022-12-04T07:21:24.113204Z","shell.execute_reply":"2022-12-04T07:37:34.731329Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 21250\n  Num Epochs = 1\n  Instantaneous batch size per device = 10\n  Total train batch size (w. parallel, distributed & accumulation) = 10\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2125' max='2125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2125/2125 15:20, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.131856</td>\n      <td>0.995200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.276500</td>\n      <td>0.056331</td>\n      <td>0.993333</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.276500</td>\n      <td>0.040671</td>\n      <td>0.994133</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.048000</td>\n      <td>0.031742</td>\n      <td>0.995733</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.036100</td>\n      <td>0.031429</td>\n      <td>0.993867</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.036100</td>\n      <td>0.027466</td>\n      <td>0.995200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.026400</td>\n      <td>0.026739</td>\n      <td>0.995467</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\nSaving model checkpoint to /kaggle/working/checkpoint-500\nConfiguration saved in /kaggle/working/checkpoint-500/config.json\nModel weights saved in /kaggle/working/checkpoint-500/pytorch_model.bin\nFeature extractor saved in /kaggle/working/checkpoint-500/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\nSaving model checkpoint to /kaggle/working/checkpoint-1000\nConfiguration saved in /kaggle/working/checkpoint-1000/config.json\nModel weights saved in /kaggle/working/checkpoint-1000/pytorch_model.bin\nFeature extractor saved in /kaggle/working/checkpoint-1000/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\nSaving model checkpoint to /kaggle/working/checkpoint-1500\nConfiguration saved in /kaggle/working/checkpoint-1500/config.json\nModel weights saved in /kaggle/working/checkpoint-1500/pytorch_model.bin\nFeature extractor saved in /kaggle/working/checkpoint-1500/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\nSaving model checkpoint to /kaggle/working/checkpoint-2000\nConfiguration saved in /kaggle/working/checkpoint-2000/config.json\nModel weights saved in /kaggle/working/checkpoint-2000/pytorch_model.bin\nFeature extractor saved in /kaggle/working/checkpoint-2000/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n***** Running Evaluation *****\n  Num examples = 3750\n  Batch size = 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 00:48]\n    </div>\n    "},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.026735765859484673,\n 'eval_accuracy': 0.9954666666666667,\n 'eval_runtime': 48.9278,\n 'eval_samples_per_second': 76.643,\n 'eval_steps_per_second': 7.664,\n 'epoch': 1.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"The pre-trained model contains 11 layers plus the classifier layer we added. I've tried to freeze from 1 to 10 layers close to input in the model but it doesn't seem to impact the accuracy.","metadata":{}}]}